{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kickstarter Project\n",
    "\n",
    "#### Design a Model to allow for an assessment wether a campaign for a project on KICKSTARTER will most likely succeed or fail. This could help projectowners decide if the work connected to maintaining a campaign is worth it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be found [here](https://neuefische-students.slack.com/files/U01BHAYB2CU/F06E67VRYMB/kickstarter_data.zip) and should be saved in a folder with the name Kickstarter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Dataset \n",
    "\n",
    "The Dataset used in the course of this project consisted of data collected between April 2009 and March 2019. In general 209222 projects from 157326 creators are included in the dataset. \n",
    "\\\n",
    "Columns possibly relevant regarding the success or failiure of a project include \n",
    "\n",
    "* backers_count: amount of people pledging money to the project                                    \n",
    "* category: json string containing information on e.g. the projects id, categories and URL\n",
    "* country: country of the projects creator \n",
    "* deadline: end date of the project \n",
    "* goal: information on the amount of money needed to succeed in the local currency of the project\n",
    "* launched_at: start date of the project\n",
    "* spotlight: feature enabled after reaching the goal, to show of the project\n",
    "* staff_pick: marked by a staff member of kickstarter\n",
    "* state: (successful/failed/canceled/live/suspended)\n",
    "\n",
    "Further columns include information on the projects currency and exchange rates, as well as the creator, their specific location and their profile information. Out of these columns, only the static_usd_rate was included in order to transform the goal's currency to USD allowing for better comparability. \n",
    "\n",
    "As our model aims at predicting a campaigns success before it is started, columns including information not available to the potential creator up front had to be excluded. Those columns are the _'backers_count'_ and the _'staff_pick'_. Furthermore, _'spotlight'_ is excluded as the information is identical to the information given in the _'state'_ column, which will be used as a target column in the following.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important libraries \n",
    "\n",
    "import pandas as pd \n",
    "import os, re, json, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from utils.evaluation import *                  #contains functions used for model evaluation\n",
    "from utils.Preprocessing_funct import *         #contains functions used for pre-processing\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data \n",
    " The file *Preprocessing_funct.py* contains all functions that were used in the Preprocessing. Reasons for the application of the individual functions are found in the comments behind the functions, or in the 'EDA.ipynb' notebook.\n",
    " \n",
    " The following cell contains a workflow to load, preprocess, clean and save the data. Alternatively the dataloader.py can be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################################################################################\n",
    "# ###### The preprocessed Data is saved under 'data/cleaned_data.csv' ################################################\n",
    "# ####################################################################################################################\n",
    "\n",
    "\n",
    "# ############# Load the data into a dataframe #######################################################################\n",
    "\n",
    "# directory = 'Kickstarter_data/'\n",
    "# data = pd.DataFrame()\n",
    "# relevant_columns = ['category', 'country', 'creator', 'state', 'static_usd_rate', 'goal', 'launched_at', 'deadline']\n",
    "\n",
    "# for file in sorted(os.listdir(directory)):\n",
    "#     df_temp = pd.read_csv(directory+file)\n",
    "#     data = pd.concat([data, df_temp[relevant_columns]], ignore_index=True)\n",
    "\n",
    "# ############ clean data of duplicates ################################################################################\n",
    "\n",
    "# data = data.drop_duplicates(ignore_index =True)     \n",
    "\n",
    "# ############## transform columns containing dates to datetime ##########################################################\n",
    "\n",
    "# data['launched_at'] = pd.to_datetime(data['launched_at'], unit='s')\n",
    "# data['deadline'] = pd.to_datetime(data['deadline'], unit='s')\n",
    "\n",
    "# ############## preprocess the columns ##################################################################################\n",
    "\n",
    "# data = state_to_binary(data)                        # work on state-column (to binary -> failed/succeeded)\n",
    "# data = extract_category(data)                       # extract the category of the project\n",
    "# data = extract_year_date_month(data, 'launched_at') # extract year and date of the launched_at column\n",
    "# data = duration(data, 'launched_at', 'deadline')    # extract campaign duration from launched_at and deadline\n",
    "# data = convert_to_usd(data)                         # convert goal column to USD for unification \n",
    "# data = north_america(data)                          # work on country column to devide between north america and not north america\n",
    "# data = unrealistic_goal(data)                       # exclude goals above 1000000 USD, see EDA notebook\n",
    "\n",
    "# ########### drop unneccessary columns ####################################################################################\n",
    "\n",
    "# data = data.drop(['category', 'country', 'creator', 'static_usd_rate', 'launched_at', 'deadline'], axis =1)\n",
    "\n",
    "# ########### Label-encode the categorical columns containing strings ######################################################\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# data['slug'] = le.fit_transform(data['slug'])\n",
    "\n",
    "# ##########  save pre-processed Dataframe in csv_file #####################################################################\n",
    "\n",
    "# data.to_csv('data/cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Target from Features, perform Train-Test split and scale data\n",
    "\n",
    "While some models tested in the course of the project require Standard scaled values, others require MinMax scaled values for good training results. Here we introduce a standard scaled and a minmax scaled train and test set, that can be used for the different models besides the unscaled dataset. Further, for the scaled datasets, the categorical data is one-hot-encoded. The scaled train and test datasets generated in the cell below were saved in the folder 'data'. To uncomment the cell, click inside the cell, press cmd+a, then cmd+k and cmd+u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################################################################################################\n",
    "# #### The data generated in this cell can be found in the data folde. Only execute when aiming at new train/test sets ######\n",
    "# ###########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# ########## Load Data ######################################################################################################\n",
    "# data = pd.read_csv('data/cleaned_data.csv')\n",
    "\n",
    "# ########## define target and features variables ###########################################################################\n",
    "# X = data.drop('state', axis=1)\n",
    "# y = data.state\n",
    "\n",
    "# ########## split to train and test ########################################################################################\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=RSEED)\n",
    "\n",
    "# ########## define categorical and numerical columns #######################################################################\n",
    "# cat_columns = ['slug', 'launched_at_weekday', 'launched_at_month']\n",
    "# num_columns = ['goal', 'duration_days']\n",
    "\n",
    "# ########## One hot encode categorical data ################################################################################\n",
    "# X_train_oh = pd.get_dummies(X_train, columns = cat_columns, drop_first = True)\n",
    "# X_test_oh = pd.get_dummies(X_test, columns = cat_columns, drop_first = True)\n",
    "\n",
    "# ########## Apply standard scaler on numerical data #######################################################################\n",
    "# std_scaler = StandardScaler()\n",
    "# X_train_stdscaled = std_scaler.fit_transform(X_train[num_columns])\n",
    "# X_test_stdscaled = std_scaler.transform(X_test[num_columns])\n",
    "\n",
    "# X_train_stdscaled = pd.DataFrame(np.concatenate([X_train_stdscaled, X_train_oh.drop(num_columns, axis=1)], axis=1))\n",
    "# X_test_stdscaled = pd.DataFrame(np.concatenate([X_test_stdscaled, X_test_oh.drop(num_columns, axis=1)], axis=1))\n",
    "\n",
    "# ########## Apply MinMax scaler on numerical data #########################################################################\n",
    "# mm_scaler = MinMaxScaler()\n",
    "# X_train_mmscaled = mm_scaler.fit_transform(X_train_oh[num_columns])\n",
    "# X_test_mmscaled = mm_scaler.transform(X_test_oh[num_columns])\n",
    "\n",
    "# X_train_mmscaled = pd.DataFrame(np.concatenate([X_train_mmscaled, X_train_oh.drop(num_columns, axis=1)], axis=1))\n",
    "# X_test_mmscaled = pd.DataFrame(np.concatenate([X_test_mmscaled, X_test_oh.drop(num_columns, axis=1)], axis=1))\n",
    "\n",
    "\n",
    "# ########## Save the different x_train & x_test versions ##################################################################\n",
    "# y_train.to_csv('data/y_train.csv', index=False)\n",
    "# y_test.to_csv('data/y_test.csv', index=False)\n",
    "\n",
    "# X_train.to_csv('data/unscaled_Xtrain.csv', index=False)\n",
    "# X_test.to_csv('data/unscaled_Xtest.csv', index=False)\n",
    "\n",
    "# X_train_stdscaled.to_csv('data/stdscaled_Xtrain.csv', index=False)\n",
    "# X_test_stdscaled.to_csv('data/stdscaled_Xtest.csv', index=False)\n",
    "\n",
    "# X_train_mmscaled.to_csv('data/mmscaled_Xtrain.csv', index=False)\n",
    "# X_test_mmscaled.to_csv('data/mmscaled_Xtest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Models\n",
    "\n",
    "In this notebook our best model after investigating multiple machine learning algorithms, including **logistic regression**, **k-nearest neighbours**, **random forest**, **xgboost** and **stacking** of different models, with grid search is shown. The best model was a **XGBoost** model with the following parameters: .\n",
    "Further, the baseline model (logistic regression with baseline parameters) is shown.\n",
    "The grid searches for the different models are shown in the 'Gridsearch_models.ipynb' notebook. The best models for each algorithms are saved in the models folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaled train and test data (standard scaled for logistic regression and original data for xgboost)\n",
    "\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv')\n",
    "\n",
    "X_train = pd.read_csv('data/unscaled_Xtrain.csv')\n",
    "X_test = pd.read_csv('data/unscaled_Xtest.csv')\n",
    "\n",
    "X_train_stdscaled = pd.read_csv('data/stdscaled_Xtrain.csv')\n",
    "X_test_stdscaled = pd.read_csv('data/stdscaled_Xtest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Baseline model\n",
    "\n",
    "As a baseline model, a simple logistic regression, using the default parameters (penalty='l2', C=1.0, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto') was performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train baseline model\n",
    "# logreg_baseline = LogisticRegression()      \n",
    "# logreg_baseline.fit(X_train, y_train)\n",
    "\n",
    "# # Save baseline model\n",
    "# pickle.dump(logreg_baseline, open('./models/baseline_logreg_model', 'wb')) \n",
    "\n",
    "# Load baseline model\n",
    "logreg_baseline = pickle.load(open('./models/baseline_logreg_model', 'rb'))\n",
    "\n",
    "# Visualise Results\n",
    "y_pred_baseline = logreg_baseline.predict(X_test)\n",
    "vis_results(y_test, y_pred_baseline, 'Baseline_LogReg', savefig=False)\n",
    "\n",
    "# get baseline accuracy in percent\n",
    "base_line_acc = accuracy_score(y_test, y_pred_baseline)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Best model\n",
    "\n",
    "The best model was a XGBoost model with the following parameters: 'colsample_bytree': 0.85, 'learning_rate': 0.1, 'max_depth': 23, 'n_estimators': 100, 'subsample': 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pickle.load(open('./models/best_xgb_model', 'rb'))\n",
    "\n",
    "# Predict on best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Plot classification report and confusion matrix\n",
    "vis_results(y_test, y_pred_best, 'XGBoost', savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare accuracies\n",
    "\n",
    "As the statistics knowledge of our clients (small business owners) might vary, the accuracy was chosen as a metric that can be understood intuitively and moreover, is relatively save, as we optimized the models on precision and can therefore exclude the risk of having a high accuracy while precision is rather low. Nevertheless, confusion matrices were saved for each model to be shown to clients interested in more in-depth statistics.\\\n",
    "In order to compare the accuracies a bar plot appeared to be the most comprehensible approach, easy to grasp with just one glance in the presentation. Additionally the baseline accuracy was plotted for comparison reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pickle.load(open('accuracies.txt', 'rb'))\n",
    "accuracies = pd.DataFrame(accuracies.items(), columns=['model', 'accuracy'])\n",
    "accuracies.accuracy = accuracies.accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrs = ['grey' if (x < accuracies.accuracy.max()) else 'red' for x in accuracies.accuracy]\n",
    "ax = sns.barplot(x='model', y='accuracy', data=accuracies, palette=clrs, saturation=0.5)\n",
    "ax.axhline(base_line_acc, color=\"black\", linestyle = 'dashed', label='baseline')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Model')\n",
    "ax.legend()\n",
    "ax.figure.savefig('images/accuracy_comparison.png',dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis \n",
    "\n",
    "Finally, a small error analysis was performed for the results of the best model (XGBoost, as seen above). The results are shown in the **Error_analysis.ipynb** notebook. However, no specific errors were found. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
