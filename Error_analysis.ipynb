{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "In this notebook we want to get an impression wether there are some patterns in the data that was misclassified by our best XGBoost model (see General.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see patterns from the predictions, a column indicating wether the prediction of a datapoint was a true positive, a true negative, a false positive or a false negative was created. The results were visualised in a pairplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "y_test = pd.read_csv('data/y_test.csv')\n",
    "X_test = pd.read_csv('data/unscaled_Xtest.csv')\n",
    "\n",
    "# Load best model\n",
    "best_model = pickle.load(open('./models/best_xgb_model', 'rb'))\n",
    "\n",
    "# Predict on best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Reshape test labels to 1D array\n",
    "y_test = np.array(y_test)\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "# Array of True and False marking correct/incorrect predictions\n",
    "corr_pred = y_test == y_pred\n",
    "\n",
    "X_test_analysis = pd.DataFrame(X_test)\n",
    "X_test_analysis['corr_pred'] = corr_pred\n",
    "X_test_analysis['true_res'] = y_test\n",
    "\n",
    "\n",
    "# Identifying wether prediction is True Positive (TP), True Negative (TN), False Positive (FP) or False Negative (FN)\n",
    "result_category = np.empty(len(y_test), dtype=object)\n",
    "mtp = (corr_pred == True)\n",
    "mfp = (corr_pred == False)\n",
    "mtt = (y_test == 1)\n",
    "mft = (y_test == 0)\n",
    "\n",
    "mTP = np.logical_and(mtp, mtt)\n",
    "mFP = np.logical_and(mtp, mft)\n",
    "mTN = np.logical_and(mfp, mft)\n",
    "mFN = np.logical_and(mfp, mtt)\n",
    "\n",
    "result_category[mTP] = 'TP'\n",
    "result_category[mFP] = 'FP'\n",
    "result_category[mTN] = 'TN'\n",
    "result_category[mFN] = 'FN'\n",
    "\n",
    "X_test_analysis['result_category'] = result_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_test_analysis.drop(['corr_pred', 'true_res'],axis=1), hue='result_category')\n",
    "plt.savefig('images/Error_Analysis/pairplot_predicted_results.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall there is no pattern visible indicating why the model failed on specific datapoints. However, regarding the false positives, it becomes clear, that some statements concluded from the EDA should be taken into consideration, next to what the model predicted. This includes \n",
    "\n",
    "- Too high goals increase the risk of a failed campaign \n",
    "- Campaigns launched in the summer months (especially July) have a higher risk of failing then campaigns launched in other months \n",
    "- Campaigns with a duration of approximately 60 days (2 month) have a higher risk of failing \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
